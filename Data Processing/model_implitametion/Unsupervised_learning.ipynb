{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eK8yqGq7QLw_"
      },
      "source": [
        "Info-284 Group exam\n",
        "Group members: Heejung Yu, Tsz Ching, Sverre-Emil and Aaron Male"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7G2yJwiQLxC"
      },
      "source": [
        "# Table of Contents\n",
        "1. [Introduction](#Introduction)\n",
        "2. [Preprocessing](#Preprocessing)\n",
        "    - [Features](#Features)\n",
        "        - [Species](#Species)\n",
        "        - [Equipment](#Equipment)\n",
        "        - [Gross Weight of Catch](#Gross-Weight-of-Catch)\n",
        "        - [Boat Information](#Boat-Information)\n",
        "        - [Location of Trip](#Location-of-Trip)\n",
        "        - [Drag Distance](#Drag-Distance)\n",
        "        - [Duration](#Duration)\n",
        "        - [Time](#Time)\n",
        "    - [Analyzation](#Analyzation)\n",
        "        - [Hovedart FAO](#Hovedart-FAO)\n",
        "        - [Lengdegruppe](#Lengdegruppe)\n",
        "        - [Redskap FAO](#Redskap-FAO)\n",
        "        - [Rundvekt](#Rundvekt)\n",
        "3. [Supervised Learning](#Supervised-Learning)\n",
        "    - [Picking the Machine Learning Models](#Picking-the-Machine-Learning-Models)\n",
        "        - [K-NN](#k-NN)\n",
        "        - [Linear Models](#Linear-Models)\n",
        "        - [Naive Bayes](#Naive-Bayes)\n",
        "        - [Decision Trees](#Decision-Trees)\n",
        "        - [Ensembles of Decision Trees](#Ensembles-of-Decision-Trees)\n",
        "        - [Neural Networks](#Neural-Networks)\n",
        "    - [Our Choices](#Our-Choices)\n",
        "4. [Unsupervised Learning](#unsupervised-learning)\n",
        "    - [Preprocessing for Unsupervised Learning](#preprocessing-for-unsupervised-learning)\n",
        "    - [Picking the Machine Learning Models for Unsupervised Learning](#picking-the-machine-learning-models-for-unsupervised-learning)\n",
        "      - [Two Kinds of Unsupervised Learning Algorithms](#two-kinds-of-unsupervised-learning-algorithms)\n",
        "      - [Picking a Clustering Algorithm](#picking-a-clustering-algorithm)\n",
        "    - [Clustering with DBSCAN](#-clustering-with-dbscan)\n",
        "      - [Looking into the Influence of the Values of Parameters](#looking-into-the-influence-of-the-values-of-parameters)\n",
        "      - [Is the Largest Cluster Related to Bycatch?](#is-the-largest-cluster-related-to-bycatch?)\n",
        "        - [Find a Relationship with Bycatch and Make a Confusion Matrix](#find-a-relationship-with-bycatch-and-make-a-confusion-matrix)\n",
        "      -[Reflection on Unsupervised Learning](#reflection-on-unsupervised-learning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47qY17GaQLxD"
      },
      "source": [
        "## Introduction\n",
        "After taking a quick look at the dataset and the documents that was provided with it, we figured we wanted to try to predict if an entry is a Bycatch. We believe that by being able to predict Bycatch we can find out if there are any boats that are misreporting their catches. We classify an entry as a Bycatch if the \"Hovedart FAO\" does not match with \"Art FAO\". We are aware this definition of a Bycatch is somewhat limited especially considering the way \"Hovedart FAO\" is chosen, ideally we would like to classify Bycatch as non-intended catches. However, this is hard to do without information about the fishing vessel's objectives prior to departure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgG08aHKQLxE"
      },
      "outputs": [],
      "source": [
        "import warnings # Got an irritating warning\n",
        "warnings.filterwarnings('ignore')\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "pd.set_option('display.max_columns',None)\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('float_format', '{:f}'.format)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2Ub4yCKQLxF"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv(\"/content/elektronisk-rapportering-ers-2018-fangstmelding-dca-simple.csv\", sep = \";\")\n",
        "# Dataset where the species isn't the same as the main-species\n",
        "bycatch = dataset[dataset['Art FAO'] != dataset['Hovedart FAO']]\n",
        "main_species = dataset[dataset['Art FAO'] == dataset['Hovedart FAO']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BasTf5cEQLxF"
      },
      "source": [
        "## Preprocessing\n",
        "\n",
        "### Picking features\n",
        "When choosing features to use from the dataset, we divided the 45 different columns into 8 different categories.\n",
        "<br>\n",
        "<br>\n",
        "The 8 categories are as follows:\n",
        "<ul>\n",
        "<li>Species</li>\n",
        "<li>Equipments used</li>\n",
        "<li>Gross weight of catch</li>\n",
        "<li>Boat information</li>\n",
        "<li>Location of trip</li>\n",
        "<li>Drag distance</li>\n",
        "<li>Duration</li>\n",
        "<li>Time</li>\n",
        "</ul>\n",
        "\n",
        "We chose to use the following features in our ML model: art, hovedart, redskap, rundvekt, lengdegruppe, hovedomr√•de.\n",
        "\n",
        "Our methodology in picking the features was first seeing how the data could be relevant for our prediction, we then plotted the data from each feature into various graphs to see the distribution and also looked at the relationship between one feature and the other."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Spgf9RneQLxG"
      },
      "source": [
        "#### Species\n",
        "\n",
        "This feature consisted mainly of \"Hovedart FAO\" and \"Art FAO\", this feature is a necessity as it is used to check if the species is classified as a Bycatch or not."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3JJ7mQyQLxG"
      },
      "source": [
        "#### Equipments\n",
        "\n",
        "How can we measure the usefulness of equipments when predicting Bycatch? We checked the distribution of equipments in the original dataset and compared it with the Bycatch dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "uls4OQr3QLxG",
        "outputId": "ab4329c6-647b-4462-89ba-d0c8c74d1a24"
      },
      "outputs": [],
      "source": [
        "# A count of instances of equipment used for every Bycatch.\n",
        "count_of_equipment_used_for_only_Bycatch = bycatch.groupby([\"Redskap FAO\"])[\"Redskap FAO\"].count()\n",
        "count_of_equipment_used_for_only_Bycatch = count_of_equipment_used_for_only_Bycatch.sort_values(ascending=False)\n",
        "count_of_equipment_used_for_only_Bycatch.plot(kind=\"bar\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "vSIOTCf8QLxH",
        "outputId": "a89924a5-73e4-4fbc-c5b5-e7dd64fcb690"
      },
      "outputs": [],
      "source": [
        "# A count of instances of equipment used for every species.\n",
        "count_of_equipment_used_for_original_dataset = dataset.groupby([\"Redskap FAO\"])[\"Redskap FAO\"].count()\n",
        "count_of_equipment_used_for_original_dataset = count_of_equipment_used_for_original_dataset.sort_values(ascending=False)\n",
        "count_of_equipment_used_for_original_dataset.plot(kind=\"bar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tj3eCa4OQLxH"
      },
      "source": [
        "Although there were a few difference in the distribution, the Bycatch dataset had a more noticeable sections whereas the original dataset had a steady decline. We didn't find there were too much of a difference between the two, and were unsure if it was relevant. So we tried another approach, we checked the correlation between the most common Bycatch species and their equipment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "dAr_Yh4RQLxI",
        "outputId": "5cee1696-406c-4ea6-8509-fb8246cb1e43"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Finding the most common bycatch, defined by Bycatch species with highest roundweight.\n",
        "count_of_Bycatches_for_every_main_species = bycatch.groupby([\"Art FAO\"])[\"Rundvekt\"].sum()\n",
        "\n",
        "# Top 5 species\n",
        "top_5_common_bycatch = (count_of_Bycatches_for_every_main_species.sort_values(ascending=False))[:5]\n",
        "top_5_common_bycatch = list(top_5_common_bycatch.index)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(16, 12))\n",
        "\n",
        "positions = np.arange(len(top_5_common_bycatch))*3\n",
        "width = 0.5\n",
        "\n",
        "# Finding the most common equipment used for catching each of these species\n",
        "for i, species in enumerate(top_5_common_bycatch):\n",
        "    species_only_dataset = bycatch[bycatch[\"Art FAO\"] == species]\n",
        "    count_of_equipment_used = species_only_dataset.groupby(\"Redskap FAO\")[\"Redskap FAO\"].count()\n",
        "    top_equipment_for_species = count_of_equipment_used.sort_values(ascending=False).head(5)\n",
        "\n",
        "    for j, equipment in enumerate(top_equipment_for_species.index):\n",
        "        ax.bar(positions[i] + j*width, top_equipment_for_species[equipment], width, label=f'{species} - {equipment}')\n",
        "\n",
        "ax.set_xlabel('Species and Equipment')\n",
        "ax.set_ylabel('Count')\n",
        "ax.set_title('Top Bycatch Species and Their Most Common Equipment')\n",
        "\n",
        "ax.set_xticks(positions + width)\n",
        "ax.set_xticklabels(top_5_common_bycatch)\n",
        "\n",
        "plt.legend()\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMgxi9B7QLxI"
      },
      "source": [
        "Here we see that the distribution of most common equipment used for each species varies a lot, although the top equipments for the species are the same, the rest of the equipments varies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paYujZDDQLxJ"
      },
      "source": [
        "#### Gross weight of catch\n",
        "\n",
        "This one is relevant because of the way main species is calculated; \"Main species caught, reported using the FAO species code. Main species is chosen using highest estimated weight in round kilograms.\" (datadokumentasjon-ers-rapport-varnivaa-5-140121, p.11) This means that the main species caught on average will be higher than if it were a Bycatch.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RxBBJk1_QLxJ"
      },
      "source": [
        "#### Boat information\n",
        "\n",
        "We chose to use Length group because the data in this feature is already categorized which makes it easier to process and use. We believe that the boat size is relevant when we use it together with equipments as we believe bigger boats use trawl equipments more often. We checked the distribution of equipments for every boat size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W70SVcKPQLxJ",
        "outputId": "93bc2181-11cf-4990-f3bb-b9739416e5fa"
      },
      "outputs": [],
      "source": [
        "every_lengthgroup = [\"28 m og over\", \"21-27,99 m\", \"15-20,99 m\"]\n",
        "\n",
        "for length_group in every_lengthgroup:\n",
        "        lengdegruppe_dataset = dataset[dataset[\"Lengdegruppe\"] == length_group]\n",
        "        lengdegruppe_equipmentcount = lengdegruppe_dataset.groupby([lengdegruppe_dataset[\"Redskap FAO\"]])[\"Redskap FAO\"].count()\n",
        "        lengdegruppe_equipmentcount.sort_values(ascending=False, inplace=True)\n",
        "        print(f\"Top 3 common equipment for boats in category: {length_group}\\n{lengdegruppe_equipmentcount.head(3)}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opQyyYADQLxJ"
      },
      "source": [
        "We see that the distribution of equipments used in bigger boats is much more skewed towards trawls than the distribution in smaller boat. Therefore we believe that bigger boats are more likely to result in Bycatch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fMCuq6pMQLxK"
      },
      "source": [
        "#### Location of trip\n",
        "\n",
        "To check if the area is relevant we chose to look at a Bycatch and main species dataset for torsk (Because it is the most common species). We compared the areas where they were caught as a Bycatch and where they were caught as the main species."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5x9iFstQLxK"
      },
      "outputs": [],
      "source": [
        "torsk_only = dataset[dataset['Art FAO'] == \"Torsk\"]\n",
        "torsk_only_Bycatch = torsk_only[torsk_only['Hovedart FAO'] != \"Torsk\"]\n",
        "torsk_only_main = torsk_only[torsk_only['Hovedart FAO'] == \"Torsk\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "kP5mP2QJQLxK",
        "outputId": "40494149-2695-462a-a171-95bed5732ad1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Count the occurrences of each unique value in 'Hovedomr√•de start', sort, and select top 10\n",
        "top_10_areas_bycatch_start = torsk_only_Bycatch['Hovedomr√•de start'].value_counts().head(10)\n",
        "\n",
        "# Plot the top 10 areas\n",
        "top_10_areas_bycatch_start.plot(kind='pie', figsize=(12, 10), fontsize=10, autopct='%1.1f%%', startangle=140)\n",
        "\n",
        "plt.title('Top 10 Torsk Bycatch Main Start Areas', fontsize=12)\n",
        "plt.ylabel('')\n",
        "plt.legend(title='Main Start Areas', loc='upper right', bbox_to_anchor=(0, 1))\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "0oL6aD-rQLxL",
        "outputId": "7d143f7d-c702-46ce-d0f8-b09c4c753540"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Count the occurrences of each unique value in 'Hovedomr√•de start', sort, and select top 10\n",
        "top_10_areas = torsk_only_main['Hovedomr√•de start'].value_counts().head(10)\n",
        "\n",
        "# Plot the top 10 areas\n",
        "top_10_areas.plot(kind='pie', figsize=(12, 10), fontsize=10, autopct='%1.1f%%', startangle=140)\n",
        "\n",
        "plt.title('Top 10 Torsk Non-Bycatch Main Start Areas', fontsize=12)\n",
        "plt.ylabel('')\n",
        "plt.legend(title='Main Start Area', loc='upper right', bbox_to_anchor=(0, 1))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FxIveCNCQLxL"
      },
      "source": [
        "There is interestingly a big difference where the fish is caught when we compare Bycatch with main species."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fI9je5sWQLxL"
      },
      "source": [
        "#### Drag distance\n",
        "\n",
        "To check if drag distance is relevant when predicting Bycatch we just took a quick look and compared values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHOZzbgDQLxL",
        "outputId": "f41e1cd0-5f42-40b7-b0a2-768aa533a693"
      },
      "outputs": [],
      "source": [
        "print(\"Duration for original dataset:\\n\", dataset[\"Trekkavstand\"].describe())\n",
        "print(\"\\n\")\n",
        "print(\"Duration for bycatch:\\n\",bycatch[\"Trekkavstand\"].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r7VjyQnhQLxM"
      },
      "source": [
        "Doesn't look like there is anything special here, maybe we can check drag distance for torsk when a specific equipment is used? We used the most common equipment for catching torsk: Bunntr√•l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1SOpzvy2QLxM"
      },
      "outputs": [],
      "source": [
        "torsk_only_dragdistance = dataset[dataset[\"Art FAO\"] == \"Torsk\"]\n",
        "# Removing an outlier so its easier to read the graph\n",
        "torsk_only_dragdistance = torsk_only_dragdistance[torsk_only_dragdistance[\"Trekkavstand\"] < 200000]\n",
        "\n",
        "torsk_as_Bycatch_dragdistance = torsk_only_dragdistance[torsk_only_dragdistance[\"Hovedart FAO\"] != \"Torsk\"]\n",
        "torsk_as_Bycatch_dragdistance = torsk_as_Bycatch_dragdistance[torsk_as_Bycatch_dragdistance[\"Redskap FAO\"] == \"Bunntr√•l, otter\"]\n",
        "\n",
        "\n",
        "torsk_as_main_dragdistance = torsk_only_dragdistance[torsk_only_dragdistance[\"Hovedart FAO\"] == \"Torsk\"]\n",
        "torsk_as_main_dragdistance = torsk_as_main_dragdistance[torsk_as_main_dragdistance[\"Redskap FAO\"] == \"Bunntr√•l, otter\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "A1yc-UF2QLxM",
        "outputId": "19fe766b-9b0d-418f-d30d-c0c1c0eea233"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Scatter plot for bycatch\n",
        "plt.scatter(torsk_as_Bycatch_dragdistance[\"Rundvekt\"], torsk_as_Bycatch_dragdistance[\"Trekkavstand\"], color='blue', alpha=0.5, label='Torsk as Bycatch')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel(\"Gross Weight (kg)\", fontsize=14)\n",
        "plt.ylabel(\"Distance Traveled (km)\", fontsize=14)\n",
        "plt.title(\"Amount of Torsk Caught per Distance Traveled\", fontsize=16)\n",
        "\n",
        "# Adding a legend to distinguish between the two datasets\n",
        "plt.legend(title=\"Catch Type\", title_fontsize='13', fontsize='12')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "St8lBE4uQLxN",
        "outputId": "f7a8ed48-becb-41ac-a0db-a479ff6e158f"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Scatter plot for main catch\n",
        "plt.scatter(torsk_as_main_dragdistance[\"Rundvekt\"], torsk_as_main_dragdistance[\"Trekkavstand\"], color='red', alpha=0.5, label='Torsk as Main Catch')\n",
        "\n",
        "# Adding labels and title\n",
        "plt.xlabel(\"Gross Weight (kg)\", fontsize=14)\n",
        "plt.ylabel(\"Distance Traveled (km)\", fontsize=14)\n",
        "plt.title(\"Amount of Torsk Caught per Distance Traveled\", fontsize=16)\n",
        "\n",
        "# Adding a legend to distinguish between the two datasets\n",
        "plt.legend(title=\"Catch Type\", title_fontsize='13', fontsize='12')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIvROHGDQLxN",
        "outputId": "02936ac8-6936-4f30-d542-e59800fbb200"
      },
      "outputs": [],
      "source": [
        "# amount of entries from 40 000 to 75 000 in Bycatch\n",
        "filtered_dragdistance = torsk_as_Bycatch_dragdistance[(torsk_as_Bycatch_dragdistance[\"Trekkavstand\"] > 40000) & (torsk_as_Bycatch_dragdistance[\"Trekkavstand\"] < 75000)]\n",
        "\n",
        "total_instances = filtered_dragdistance.shape[0]\n",
        "\n",
        "print(total_instances)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oedNeF_0QLxN"
      },
      "source": [
        "Here we see that drag distance makes very little difference, most of the dots in the scatterplot is concentrated around 50-60k. while there are a bit fewer dots in the main species graph around 60k it is only 800 entries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vm1dCzqQLxN"
      },
      "source": [
        "#### Duration\n",
        "\n",
        "To check if duration is relevant when predicting Bycatch we just took a quick look and compared values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAJoGC0UQLxN",
        "outputId": "4eb656ed-0a4d-40b1-ee9b-d3307e26b74b"
      },
      "outputs": [],
      "source": [
        "print(\"Duration for original dataset:\\n\", dataset[\"Varighet\"].describe())\n",
        "print(\"\\n\")\n",
        "print(\"Duration for bycatch:\\n\", bycatch[\"Varighet\"].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95Dryu6lQLxO"
      },
      "source": [
        "As we can see the 25, 50 and 75 percentile are more or less the same, we believe this doesn't really help us predict Bycatch anyways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsnhGk4MQLxO"
      },
      "source": [
        "#### Time\n",
        "\n",
        "To see the relevance of time for our ml model, we can compare the start times of each fishing trip for torsk where the equipment used is the same."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8TSvhewQLxP"
      },
      "outputs": [],
      "source": [
        "torsk_only_Bycatch_bunntr√•l = torsk_only_Bycatch[torsk_only_Bycatch[\"Redskap FAO\"] == \"Bunntr√•l, otter\"]\n",
        "torsk_only_main_bunntr√•l = torsk_only_main[torsk_only_main[\"Redskap FAO\"] == \"Bunntr√•l, otter\"]\n",
        "\n",
        "# Convert columns to datetime format to extract the hour\n",
        "torsk_only_Bycatch_bunntr√•l['Startklokkeslett'] = pd.to_datetime(torsk_only_Bycatch_bunntr√•l['Startklokkeslett'], format='%H:%M')\n",
        "torsk_only_main_bunntr√•l['Startklokkeslett'] = pd.to_datetime(torsk_only_main_bunntr√•l['Startklokkeslett'], format='%H:%M')\n",
        "\n",
        "# Extract the hour from the start time in both main and Bycatch dataframe\n",
        "torsk_only_Bycatch_bunntr√•l['Startklokkeslett_time'] = torsk_only_Bycatch_bunntr√•l['Startklokkeslett'].dt.hour\n",
        "torsk_only_main_bunntr√•l['Startklokkeslett_time'] = torsk_only_main_bunntr√•l['Startklokkeslett'].dt.hour\n",
        "\n",
        "# counting hours\n",
        "hourly_distribution_start = torsk_only_Bycatch_bunntr√•l['Startklokkeslett_time'].value_counts().sort_index()\n",
        "hourly_distribution_start2 = torsk_only_main_bunntr√•l['Startklokkeslett_time'].value_counts().sort_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VV7Bt-qQLxP"
      },
      "source": [
        "We chose to use a pie-chart because it visualizes the data better when we want to see percentage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "FoKsyNf-QLxP",
        "outputId": "75506b89-3de2-4c33-fa54-68d8b073b858"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "hourly_distribution_start.plot(kind='pie', autopct='%1.1f%%', startangle=90)\n",
        "plt.title('Hourly Distribution of Torsk Bycatch Start Times')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "nBS6r8R9QLxQ",
        "outputId": "f260005c-ba1c-4bf0-904f-63a6f7a3ed8a"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 8))\n",
        "hourly_distribution_start2.plot(kind='pie', autopct='%1.1f%%', startangle=90)\n",
        "plt.title('Hourly Distribution of Torsk Bycatch Start Times')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-ODiBtzQLxQ"
      },
      "source": [
        "As we can see the start times for Bycatch and main species are basically the same, this makes us believe that it doesn't really matter when the fishing is done as it doesn't impact the statistics for Bycatch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaNSykVfQLxR"
      },
      "outputs": [],
      "source": [
        "# Excluding all features that aren't relevant\n",
        "dataset = dataset[[\"Hovedart FAO\", \"Art FAO\", \"Lengdegruppe\", \"Redskap FAO\", \"Rundvekt\", \"Hovedomr√•de start\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWkHU7ZGQLxR"
      },
      "source": [
        "### Analyzation\n",
        "The primary objective of this analysis is to explore findings related to outliers within each chosen features of our dataset. By examining these outliers, we aim to understand their impact on the dataset and determine appropriate strategies for handling them. This process involves identifying outliers, assessing their significance, and deciding on actions such as keeping, modifying, or removing these outliers.\n",
        "#### Hovedart FAO\n",
        "Considering that \"Hovedart FAO\" will be the feature we use to identify if it is a Bycatch the only thing we have to consider are NaN values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0h58G_G_QLxS",
        "outputId": "74f6006f-a67b-473c-efb0-46317267176f"
      },
      "outputs": [],
      "source": [
        "print(dataset[\"Hovedart FAO\"].isna().sum())\n",
        "print(dataset[\"Art FAO\"].isna().sum())\n",
        "\n",
        "# Drop all rows where Hovedart FAO and Art FAO is NaN\n",
        "dataset = dataset.dropna(subset=[\"Hovedart FAO\", \"Art FAO\"])\n",
        "\n",
        "print(dataset[\"Hovedart FAO\"].isna().sum())\n",
        "print(dataset[\"Art FAO\"].isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQ7e5rHFQLxS"
      },
      "source": [
        "Considering that this feature will be a vital part of our ml model we will be dropping all NaN values in this feature."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jDUUO50bQLxS"
      },
      "source": [
        "#### Lengdegruppe\n",
        "Considering this feature is already categorized outliers aren't an issue here, we believe its only relevant to look at NaN values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "E-vpDYuXQLxT",
        "outputId": "27b15712-a637-4c03-8368-97b07e6d99e2"
      },
      "outputs": [],
      "source": [
        "print(dataset[\"Lengdegruppe\"].isna().sum())\n",
        "\n",
        "lengdegruppe_is_NaN = dataset[(dataset[\"Lengdegruppe\"].isna())]\n",
        "\n",
        "pd.DataFrame(lengdegruppe_is_NaN.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYeuHO2rQLxT"
      },
      "source": [
        "At first glance we were planning on dropping these values, however when we actually look at the values here we can see that most catches without a specified boat length is when Stortare is caught."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FThHs54iQLxT",
        "outputId": "d62bf030-1ff6-420d-8b35-6c89371be7ac"
      },
      "outputs": [],
      "source": [
        "art_fao_counts_lengdegruppe = lengdegruppe_is_NaN['Art FAO'].value_counts()\n",
        "\n",
        "# Printing the counts for each unique value in 'Art FAO'\n",
        "print(art_fao_counts_lengdegruppe)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W39_bnWfQLxU"
      },
      "source": [
        "Looks like rather than dropping NaN values we will be converting them into a special group, this is because all NaN values are catching Stortare."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eT7d9FUDQLxU"
      },
      "outputs": [],
      "source": [
        "dataset['Lengdegruppe'] = dataset['Lengdegruppe'].fillna('Stortare b√•ter')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phCyvmZzQLxU"
      },
      "source": [
        "#### Redskap FAO\n",
        "Looking at NaN values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "2LwqOQZeQLxV",
        "outputId": "5d98d754-751f-4640-9d53-b1331da849e5"
      },
      "outputs": [],
      "source": [
        "print(dataset[\"Redskap FAO\"].isna().sum())\n",
        "redskap_FAO_is_NaN = dataset[dataset[\"Redskap FAO\"].isna()]\n",
        "pd.DataFrame(redskap_FAO_is_NaN).head(10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Osf1kggDQLxV"
      },
      "source": [
        "Considering there are only 200 NaN values the easiest solution will be dropping them, although they have aren't missing any values other than Redskap FAO we feel that the amount of work that would be needed to impute or replace the missing values will be unnoticeable in the ml models accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-VrSsoRKQLxV"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.dropna(subset=[\"Redskap FAO\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7NLil3WiQLxV"
      },
      "source": [
        "#### Rundvekt\n",
        "When looking at outliers we can assume that since different species most likely have a different average gross weight we should be looking for outliers for each individual species. We chose to use box plots to visualize these because they show outliers well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "FTxekqbUQLxW",
        "outputId": "03f5c60d-656b-49b8-88d7-eb48755377b5"
      },
      "outputs": [],
      "source": [
        "species_grossweight = dataset.groupby(\"Art FAO\")[\"Rundvekt\"].apply(list)\n",
        "\n",
        "# Define the species of interest\n",
        "species_of_interest = [\"Torsk\", \"Hyse\", \"Sei\"]\n",
        "\n",
        "# Filter the aggregated data to include only the species of interest\n",
        "filtered_species_grossweight = {species: weights for species, weights in species_grossweight.items() if species in species_of_interest}\n",
        "\n",
        "# Iterate over the filtered Series using .items() for species and their corresponding gross weights\n",
        "for species, grossweights in filtered_species_grossweight.items():\n",
        "    plt.figure(figsize=(10, 6))  # Create a new figure for each species\n",
        "\n",
        "    # Create a boxplot for the species\n",
        "    plt.boxplot(grossweights)\n",
        "    plt.title(f'Boxplot of Rundvekt for {species}')\n",
        "    plt.ylabel('Rundvekt')\n",
        "    plt.xticks([1], [species])  # Set the x-tick to the name of the current species\n",
        "\n",
        "    plt.show()  # Show the plot for the current species"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvy0EWiIQLxW"
      },
      "source": [
        "Seems like there are many outliers for each species, we believe it will be difficult to outright remove outliers since there are 122 different species and there are probably some species with few outliers while others have a lot. According to \"Introduction to Machine Learning with Python: A Guide for Data Scientists\" by Andreas C. M√ºller and Sarah Guido (p.133), employing a RobustScaler offers a strategic solution to this issue. The RobustScaler effectively transforms the data by ignoring points that significantly deviate from the rest, making it particularly suitable for our dataset where outliers are prevalent but their outright removal is impractical.\"\n",
        "\n",
        "Checking for NaN values in Gross weight."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Uz8XnBBQLxW",
        "outputId": "1aad8354-102b-4ec9-ac7c-8b9a59622c7a"
      },
      "outputs": [],
      "source": [
        "print(dataset[\"Rundvekt\"].isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZKjmcKcQLxW"
      },
      "source": [
        "#### Startomr√•de\n",
        "Checking for NaN values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGsu40hoQLxW",
        "outputId": "a09cdbf4-24c6-4b3c-b898-62f983e24c6c"
      },
      "outputs": [],
      "source": [
        "print(dataset[\"Hovedomr√•de start\"].isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMUPhV-XQLxW"
      },
      "source": [
        "There are only 4000 NaN values, we believe its fine if we drop these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jie2dyY9QLxW"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.dropna(subset=[\"Hovedomr√•de start\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7aIcBkDQLxX"
      },
      "source": [
        "## Supervised learning\n",
        "In \"Introduction to Machine Learning with Python\", the book discusses seven different machine learning models: k-Nearest Neighbors (k-NN), linear models, Naive Bayes, decision trees, ensembles of decision trees and neural networks. We will be taking a look at each of these models before determining which model is most appropriate for our problem.\n",
        "\n",
        "### Picking the Machine learning models\n",
        "Considering we have 5 features, where 4 of them are categorical and 1 is continuous data, we will have to keep this distribution in mind when picking out models. Our problem is a Binary classification problem, we will therefore not be considering regression models.\n",
        "\n",
        "#### k-NN\n",
        "The k-Nearest Neighbors (k-NN) algorithm makes predictions based on the closest data points in the training dataset. k-NN algorithms struggle when there are a lot of features involved, we only have 5 features in our dataset which makes k-NN sound pretty promising.\n",
        "\n",
        "#### Linear models\n",
        "Linear models make a prediction using a linear function of the input features. Considering we are working with a classification problem, a logistic regression model seems like a viable and simple option.\n",
        "\n",
        "#### Naive Bayes\n",
        "We can rule out Bernoulli naive bayes model because its impossible for us to convert our data to binary, without ending up with 400 features. We can also rule out gaussian since it need all features to be real numbers.\n",
        "\n",
        "MultinomialNB assumes count data, we would have to change our features such that it represents an integer count. We would have to put a lot of work into feature engineering, which would mean more work when preprocessing.\n",
        "\n",
        "#### Ensembles of decision trees\n",
        "\"Decision trees are widely used models for classification and regression tasks. Essentially, they learn a hierarchy of if/else questions, leading to a decision.\"-\"Introduction to Machine Learning with Python: A Guide for Data Scientists\" by Andreas C. M√ºller and Sarah Guido (p.70)\n",
        "\n",
        "Ensembles combines multiple machine learning models, this is to better generalize the data even if each model overfit. this model generates many different trees (which will all overfit), it then uses the average or voting to decide on a value.\n",
        "\n",
        "Considering we have over 100 features and 300 000 entries, the most logical choice would be to go with gradient boosting. Since gradient boosting prunes to make smaller trees, this will make the model train much faster.  \n",
        "\n",
        "\n",
        "#### Neural networks\n",
        "Is a class of machine learning algorithms that works well with large datasets. They consist of layers of interconnected nodes or neurons, where each connection represents a weighted pathway, and nodes process inputs using activation functions to produce outputs.\n",
        "\n",
        "### Our choices\n",
        "We will be using the following models for supervised learning; Logistic regression, gradient boosting forest, neural network. Because of the aforementioned reasons we believe they are the most logical considering we are working with a classification problem, and our features consist of many categories.\n",
        "\n",
        "### Transforming dataset\n",
        "\n",
        "#### Target variable\n",
        "We are attempting to predict whether a specific catch is Bycatch and will therefore create a new column to serve as our target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJTF_XNdQLxX"
      },
      "outputs": [],
      "source": [
        "dataset[\"Is_Bycatch\"] = (dataset[\"Hovedart FAO\"] != dataset[\"Art FAO\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a-E7N6DmQLxX"
      },
      "source": [
        "#### Changing categorical data into numeric data\n",
        "We have 5 different categorical data, each of the feature has multiple different categories. One hot Encoding is not a viable options due to the sheer amount of categories we are working with. We will be using \"cat.codes\" to convert them to numeric values. We believe it isn't important to make sure that \"Hovedart FAO\" and \"Art FAO\" have the same categorical code due to the fact that we have the \"Is_Bycatch\" feature that we will use as our target variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FN3JMrHUQLxX"
      },
      "outputs": [],
      "source": [
        "# Changing categorical data into numeric data using \"cat.codes\"\n",
        "dataset[\"Art FAO\"] = dataset[\"Art FAO\"].astype(\"category\")\n",
        "dataset[\"Art FAO Codes\"] = dataset[\"Art FAO\"].cat.codes\n",
        "\n",
        "dataset[\"Lengdegruppe\"] = dataset[\"Lengdegruppe\"].astype(\"category\")\n",
        "dataset[\"Lengdegruppe Codes\"] = dataset[\"Lengdegruppe\"].cat.codes\n",
        "\n",
        "dataset[\"Hovedomr√•de start\"] = dataset[\"Hovedomr√•de start\"].astype(\"category\")\n",
        "dataset[\"Hovedomr√•de start Codes\"] = dataset[\"Hovedomr√•de start\"].cat.codes\n",
        "\n",
        "dataset[\"Redskap FAO\"] = dataset[\"Redskap FAO\"].astype(\"category\")\n",
        "dataset[\"Redskap FAO Codes\"] = dataset[\"Redskap FAO\"].cat.codes\n",
        "\n",
        "# Excluding the old features\n",
        "dataset = dataset[[\"Art FAO Codes\", \"Lengdegruppe Codes\", \"Redskap FAO Codes\", \"Rundvekt\", \"Hovedomr√•de start Codes\", \"Is_Bycatch\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1yw5QkdWg8k"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6KPU0qfQLxX"
      },
      "source": [
        "#### Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVGhbcJnQLxY"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting dataset\n",
        "X = dataset[[\"Art FAO Codes\", \"Lengdegruppe Codes\", \"Redskap FAO Codes\", \"Rundvekt\", \"Hovedomr√•de start Codes\"]]\n",
        "y = dataset[\"Is_Bycatch\"]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bE-h-phyQLxY"
      },
      "source": [
        "#### Scaling \"Rundvekt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vhNIz-_vQLxY"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Scaling only \"Rundvekt\" and not the others\n",
        "ct = ColumnTransformer(\n",
        "    [(\"scale\", RobustScaler(), [\"Rundvekt\"])],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Fitting the scaler to the training set and not the test set to prevent data leakage\n",
        "X_train_scaled = ct.fit_transform(X_train)\n",
        "X_test_scaled = ct.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoH_s0jrQLxY"
      },
      "source": [
        "### Implementation and evaluation\n",
        "We will be evaluating each model using cross validation and a grid search.\n",
        "\n",
        "Cross validation is a technique that splits the data repeatedly and trains multiple models, this ensures that we aren't lucky or unlucky with the split of the data. We believe a stratified k-Fold Cross-Validation is the best to use in our case as the difference in entries between the most common fish and the others is large.\n",
        "\n",
        "Grid search is a technique that improves the model's generalization by tuning its parameters. Running a Grid Search with Cross-Validation will ensure that we get the best possible overview of how well the model performs.\n",
        "\n",
        "We will be measuring false negatives as a measure of success since we want to reduce the amount of misreporting happening."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alJvDKpTQLxY"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import f1_score, make_scorer\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.pipeline import Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOTIvC0mZ3XC"
      },
      "source": [
        "## Unsupervised Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2yV0mHji6nE"
      },
      "source": [
        "### Preprocessing for Unsupervised Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80TcdQznR3Us"
      },
      "source": [
        "We made a new dataset for unsupervised learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmmneRL8a1-W"
      },
      "outputs": [],
      "source": [
        "# Excluding Is_Bycatch\n",
        "dataset_unsuper = dataset[[\"Art FAO Codes\", \"Lengdegruppe Codes\", \"Redskap FAO Codes\", \"Rundvekt\", \"Hovedomr√•de start Codes\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wvi972k6cztd"
      },
      "source": [
        "We used the transformed and scaled dataset that we have used in supervised learning. We did robust scaling again to the feature ‚ÄòRundvekt‚Äô for the entire dataset as it was separated by a training set and a test set in the supervised learning step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-EswmFicg5T"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# Scaling only \"Rundvekt\" and not the others\n",
        "ct = ColumnTransformer(\n",
        "    [(\"scale\", RobustScaler(), [\"Rundvekt\"])],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Fitting the scaler to the training set and not the test set to prevent data leakage\n",
        "dataset_scaled = ct.fit_transform(dataset_unsuper)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKDutK3ejCT1"
      },
      "source": [
        "### Picking the Machine Learning Models for Unsupervised Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWtDX5Mzonm4"
      },
      "source": [
        "#### Two Kinds of Unsupervised Learning Algorithms\n",
        "\n",
        "\n",
        "1. Unsupervised Transformations   \n",
        "This algorithm is an algorithms to create a new representation of data. It is often used for dimensionality reduction or finding the parts or components that make up the data (from the textbook p.131). Those are usually used for visualization or extracting useful features. Thus we believe that it‚Äôs not the best algorithm to derive a new information from the given data, what we were trying to do.\n",
        "\n",
        "2. Clustering Algorithms   \n",
        "This algorithm makes different groups, which is called ‚Äòclusters‚Äô, within a given data. By using clustering, we thought that we could discover new relationships other than bycatch between features that we‚Äôve used in the step of supervised learning.\n",
        "\n",
        "#### Picking a Clustering Algorithm  \n",
        "There are three clustering algorithms‚Äîk-Means Clustering, Agglomeration Clustering, DBSCAN. There are two points we should consider.\n",
        "\n",
        "First, our features are mostly categorical features which has been transformed into numeric. This means that the numbers in each categorical features don‚Äôt mean continuous values, which makes it impossible to calculate the means of those values for k-Means.\n",
        "\n",
        "Second, we want to explore the dataset and don‚Äôt know how many clusters there will be. However, we should set the number of clusters as a parameter for k-Means and agglomeration clustering.\n",
        "Thus, we believe that k-Means and agglomeration clustering are not proper algorithms for our dataset and we chose to use DBSCAN for unsupervised learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPNxLtX43CP_"
      },
      "source": [
        "### Clusturing with DBSCAN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L_48pOBmTvi"
      },
      "source": [
        "There are 2 parameters in DBSCAN: min_samples and eps. Eps defines the maximum distance between two samples to be considered as part of the same neighborhood while min_samples defines the number of samples in a neighborhood for a point to be considered as a core point.\n",
        "\n",
        "Firstly, we tried to see the change of result by increasing of min_samples generally."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQf2byYPZ6O-",
        "outputId": "b3f5dd92-4c3c-4754-9200-555c028d2e1f"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "#eps = 0.5 (default)\n",
        "for min in [10, 100, 200, 500, 1000, 10000]:\n",
        "    print(\"\\nmin_samples={}\".format(min))\n",
        "    dbscan = DBSCAN(min_samples=min)\n",
        "    clusters = dbscan.fit_predict(dataset_unsuper)\n",
        "    print(\"Clusters present: {}\".format(np.unique(clusters)))\n",
        "    print(\"Cluster size:{}\".format(np.bincount(clusters+1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rddmEACCl5LB"
      },
      "source": [
        "The result meets our expectation: the increase of min_samples, the decrease of number of clusters. The number of clusters deacreased from 3764 to 0 when min_samples increase from 10 to 500.\n",
        "\n",
        "As we can see, starting from min_samples = 500, there is no cluster, except cluster -1, which means that all data points will be counted as noise. (For DBSCAN, if the data points are labeled as -1, they are counted as noise) It showed that min_samples = 500 is the maximum value for min_samples in this project.\n",
        "\n",
        "Moreover, majority of the data points is counted as noise when we examine the results of min_samples = 10 and min_samples = 100. For example, over 75% of data points are treated as noise for min_samples = 10, while over 98% of data points are counted as noise for min_samples = 100. To lessen the size of the noise, we thus attempt to deal with another parameter, eps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "l15CEqxWq0Gm",
        "outputId": "afc93a46-706e-435d-8fa7-ed6a2a5eca58"
      },
      "outputs": [],
      "source": [
        "# out of RAM --> cannot run\n",
        "\"\"\"\n",
        "for eps in [1, 10, 20, 50, 100]:\n",
        "    print(\"\\neps={}\".format(eps))\n",
        "    print(\"min_samples={}\".format(10))\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=10)\n",
        "    clusters = dbscan.fit_predict(dataset_unsuper)\n",
        "    print(\"Clusters present: {}\".format(np.unique(clusters)))\n",
        "    print(\"Cluster size:{}\".format(np.bincount(clusters+1)))\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "512NBDHr5nJt"
      },
      "source": [
        "We looked at the change of the size of noise by increasing eps. However, we cannot run the whole for loop on our computers or on a google colab; it says that 'Your session crashed after using all available RAM' when eps = 20. Therefore, we tried different possible values below 20 for eps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZyLYZKRWl6h"
      },
      "source": [
        "#### Looking into the Influence of the Values of Parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BAXOKQ7WPgh_"
      },
      "source": [
        "While we were looking at the change of noise, we noticed the size of the biggest cluster dominated the dataset. We believe it would be interesting to plot it out.\n",
        "\n",
        "To compare the changes of the size of noise more clearly, we also tried to plot a graph to see the trend.\n",
        "\n",
        "We'll look into how the clustering changes according to the parameter 'eps' while the aprameter 'min_samples' is 10, 100 and 200 individually. Then we'll look into the change according to the min_samples while the eps is fixed with 17."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JMKU08CX3zM"
      },
      "source": [
        "\n",
        "\n",
        "> min_sampels = 10\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K5zZqL3Gd-Y4",
        "outputId": "3ed038d2-92ce-4359-fb2b-8e3a84974760"
      },
      "outputs": [],
      "source": [
        "# min_samples = 10\n",
        "\n",
        "eps_values = [1, 3, 5, 7, 9, 11, 13, 15, 17]\n",
        "noise_sizes = []\n",
        "largest_cluster_sizes = []\n",
        "\n",
        "for eps in eps_values:\n",
        "    print(\"\\neps={}\".format(eps))\n",
        "    print(\"nmin_samples={}\".format(10))\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=10)\n",
        "    clusters = dbscan.fit_predict(dataset_unsuper)\n",
        "    print(\"Clusters present: {}\".format(np.unique(clusters)))\n",
        "    cluster_sizes = np.bincount(clusters + 1)\n",
        "    noise_size = cluster_sizes[0]  # Size of noise is the count of points labeled as -1\n",
        "    largest_cluster_size = np.max(cluster_sizes[1:]) if len(cluster_sizes) > 1 else 0\n",
        "    print(\"Cluster size:{}\".format(cluster_sizes))\n",
        "    print(\"Noise size: {}\".format(noise_size))\n",
        "    print(\"Largest cluster size: {}\".format(largest_cluster_size))\n",
        "    noise_sizes.append(noise_size)\n",
        "    largest_cluster_sizes.append(largest_cluster_size)\n",
        "\n",
        "# Plotting\n",
        "plt.plot(eps_values, noise_sizes, marker='o', label='Noise Size')\n",
        "plt.plot(eps_values, largest_cluster_sizes, marker='o', label='Largest Cluster Size')\n",
        "plt.title('Change in Noise and Largest Cluster Sizes for Different EPS Values')\n",
        "plt.xlabel('EPS Values')\n",
        "plt.ylabel('Size')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWv-uunfYkry"
      },
      "source": [
        "From the graph, we can see that with the increase of eps, the size of noise decreases and the size of largest cluster increase. The size of noise is inversely proportional to the value of eps, while the size of largest cluster is generally proportional to the value of eps.\n",
        "\n",
        "The size of the largest cluster sharply increased in the beginning, and continued to increase gradually when eps = 7.\n",
        "\n",
        "Additionally, we can see that starting from eps = 7, the size of noise reduced significantly to below 100000. Thus, next, we would focus on the result beginning with eps = 7."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZzdFpH7X8UZ"
      },
      "source": [
        "> min_examples = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "knX64HXcls4G",
        "outputId": "e98deb2f-e553-4827-c36f-4e40d970e404"
      },
      "outputs": [],
      "source": [
        "# min_exmaples = 100\n",
        "min = 100\n",
        "eps_values = [7, 9, 11, 13, 15, 17]\n",
        "noise_sizes = []\n",
        "largest_cluster_sizes = []\n",
        "\n",
        "for eps in eps_values:\n",
        "    print(\"\\neps={}\".format(eps))\n",
        "    print(\"nmin_samples={}\".format(min))\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=min)\n",
        "    clusters = dbscan.fit_predict(dataset_unsuper)\n",
        "    print(\"Clusters present: {}\".format(np.unique(clusters)))\n",
        "    cluster_sizes = np.bincount(clusters + 1)\n",
        "    noise_size = cluster_sizes[0]  # Size of noise is the count of points labeled as -1\n",
        "    largest_cluster_size = np.max(cluster_sizes[1:]) if len(cluster_sizes) > 1 else 0\n",
        "    print(\"Cluster size:{}\".format(cluster_sizes))\n",
        "    print(\"Noise size: {}\".format(noise_size))\n",
        "    print(\"Largest cluster size: {}\".format(largest_cluster_size))\n",
        "    noise_sizes.append(noise_size)\n",
        "    largest_cluster_sizes.append(largest_cluster_size)\n",
        "\n",
        "# Plotting\n",
        "plt.plot(eps_values, noise_sizes, marker='o', label='Noise Size')\n",
        "plt.plot(eps_values, largest_cluster_sizes, marker='o', label='Largest Cluster Size')\n",
        "plt.title('Change in Noise and Largest Cluster Sizes for Different EPS Values')\n",
        "plt.xlabel('EPS Values')\n",
        "plt.ylabel('Size')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqvPUzDmdcTw"
      },
      "source": [
        "This graph is generally similar to the previous graph. The only different is that the graph shows a smoother variation in the size of noise and largest cluster size.\n",
        "Next, we would continue to increase the min_samples to see the difference."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW_Woi2IYALI"
      },
      "source": [
        "> min_examples = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xLJ2uXWXrj_S",
        "outputId": "968062c6-ae1f-49a9-89ab-5215837f0df2"
      },
      "outputs": [],
      "source": [
        "# min_samples = 200\n",
        "min = 200\n",
        "eps_values = [7, 9, 11, 13, 15, 17]\n",
        "noise_sizes = []\n",
        "largest_cluster_sizes = []\n",
        "\n",
        "for eps in eps_values:\n",
        "    print(\"\\neps={}\".format(eps))\n",
        "    print(\"nmin_samples={}\".format(min))\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=min)\n",
        "    clusters = dbscan.fit_predict(dataset_unsuper)\n",
        "    print(\"Clusters present: {}\".format(np.unique(clusters)))\n",
        "    cluster_sizes = np.bincount(clusters + 1)\n",
        "    noise_size = cluster_sizes[0]  # Size of noise is the count of points labeled as -1\n",
        "    largest_cluster_size = np.max(cluster_sizes[1:]) if len(cluster_sizes) > 1 else 0\n",
        "    print(\"Cluster size:{}\".format(cluster_sizes))\n",
        "    print(\"Noise size: {}\".format(noise_size))\n",
        "    print(\"Largest cluster size: {}\".format(largest_cluster_size))\n",
        "    noise_sizes.append(noise_size)\n",
        "    largest_cluster_sizes.append(largest_cluster_size)\n",
        "\n",
        "# Plotting\n",
        "plt.plot(eps_values, noise_sizes, marker='o', label='Noise Size')\n",
        "plt.plot(eps_values, largest_cluster_sizes, marker='o', label='Largest Cluster Size')\n",
        "plt.title('Change in Noise and Largest Cluster Sizes for Different EPS Values')\n",
        "plt.xlabel('EPS Values')\n",
        "plt.ylabel('Size')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xpzXnbhdabX"
      },
      "source": [
        "When min_sample = 200, regardless of the change of eps, the size of noise remains greater than 100000, which is bad news because it means that 1/3 of the dataset is considered as noise.\n",
        "\n",
        "Therefore, we assumed that in order to reduce noise, min_samples have to remain below 100 with a larger value of eps.\n",
        "Thus, we take a look at how the size of noise would change with min_samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ar7kLQeDWhRl",
        "outputId": "5b3db3ec-5d33-44ef-9ff6-b4a689f1f8a6"
      },
      "outputs": [],
      "source": [
        "#eps = 17 and min_samples changes\n",
        "min_values = [10, 25, 50, 75, 100]\n",
        "eps = 17\n",
        "noise_sizes = []\n",
        "largest_cluster_sizes = []\n",
        "\n",
        "for min in min_values:\n",
        "    print(\"\\neps={}\".format(eps))\n",
        "    print(\"nmin_samples={}\".format(min))\n",
        "    dbscan = DBSCAN(eps=eps, min_samples=min)\n",
        "    clusters = dbscan.fit_predict(dataset_unsuper)\n",
        "    print(\"Clusters present: {}\".format(np.unique(clusters)))\n",
        "    cluster_sizes = np.bincount(clusters + 1)\n",
        "    noise_size = cluster_sizes[0]  # Size of noise is the count of points labeled as -1\n",
        "    largest_cluster_size = np.max(cluster_sizes[1:]) if len(cluster_sizes) > 1 else 0\n",
        "    print(\"Cluster size:{}\".format(cluster_sizes))\n",
        "    print(\"Noise size: {}\".format(noise_size))\n",
        "    print(\"Largest cluster size: {}\".format(largest_cluster_size))\n",
        "    noise_sizes.append(noise_size)\n",
        "    largest_cluster_sizes.append(largest_cluster_size)\n",
        "\n",
        "# Plotting\n",
        "plt.plot(min_values, noise_sizes, marker='o', label='Noise Size')\n",
        "plt.plot(min_values, largest_cluster_sizes, marker='o', label='Largest Cluster Size')\n",
        "plt.title('Change in Noise and Largest Cluster Sizes for Different Min_samples Values')\n",
        "plt.xlabel('Min_samples Values')\n",
        "plt.ylabel('Size')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMm5QDfeaulK"
      },
      "source": [
        "It is interesting to see a completely different situation with the previous graphs. The size of noise is proportional to the value of eps, while the size of largest cluster is inversely proportional to the value of eps.\n",
        "\n",
        "We kept eps unchanged at 17, and tried different min_samples below 100 to see the difference. It showed that the size of noise will increase slightly from 15791 to 86461, while the size of largest cluster steadily decreased from 262598 to 190603.\n",
        "\n",
        "This graph is actually out of our exceptation since we expected to see a similar graphings like the previous one. We thought it happened because the distribution of the data points is different from what we expected. And since we cannot plot the graph out with PCA (PCA usually work with continuous numbers), we know less about the distribution of dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdF_dxuQc-NK"
      },
      "source": [
        "#### Is the Largest Cluster Related to Bycatch?\n",
        "\n",
        "When we were working with the parameters, we noticed that the largest cluster commonly dominated the dataset, which contained over 2/3 of the data points. Thus, we tried to find out what this cluster stand for.\n",
        "\n",
        "Firstly, we thought of the bycatch, so we took a look at the number of bycatch and non-bycatch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJz7GBs-o8No",
        "outputId": "1b7a91b1-6081-420a-aacc-3bca97d8c13b"
      },
      "outputs": [],
      "source": [
        "# The number of bycatch in the dataset\n",
        "\n",
        "bycatch = np.sum(dataset[\"Is_Bycatch\"] == 1)\n",
        "no_bycatch = np.sum(dataset[\"Is_Bycatch\"] == 0)\n",
        "\n",
        "# Í≤∞Í≥º Ï∂úÎ†•\n",
        "print(\"Number of bycatch:\", bycatch)\n",
        "print(\"Number of non-bycatch:\", no_bycatch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pHmYMXU9qPeq"
      },
      "source": [
        "We can see that the number of bycatch and the size of the largest cluster is quite similar. Thus, we thought they may be related.\n",
        "\n",
        "Next, we tried to figure out is the largest cluster really related to bycatch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6rDQ8Zs112n"
      },
      "source": [
        "##### Find a Relationship with Bycatch and Make a Confusion Matrix\n",
        "\n",
        "We picked min_samples = 50 and eps = 17 as the parameter, as in these parameters, the size of the largest cluster is similar to the number of bycatch and the size of noise is fewer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqXqXIl9uG3q"
      },
      "source": [
        ">Hypothesis: Data points in the largest cluster is bycatch, while data points in other clusters or noise is non-bycatch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEwDmghWDcaU",
        "outputId": "a43dec76-990e-448b-ce1c-22be50055cdf"
      },
      "outputs": [],
      "source": [
        "min = 50\n",
        "eps = 17\n",
        "print(\"\\neps={}\".format(eps))\n",
        "print(\"min_samples={}\".format(min))\n",
        "dbscan = DBSCAN(eps=eps, min_samples=min)\n",
        "clusters = dbscan.fit_predict(dataset_unsuper)\n",
        "print(\"Clusters present: {}\".format(np.unique(clusters)))\n",
        "print(\"Cluster size:{}\".format(np.bincount(clusters+1)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfSACNEBxQTY"
      },
      "source": [
        "For min_samples = 50 and eps = 17, cluster [0] is the largest cluster with 209072 points. 60875 points are classified as noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zK0WOaBTtBk2"
      },
      "outputs": [],
      "source": [
        "dataset[\"Cluster\"] = clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "anw5zM_2xn1T"
      },
      "outputs": [],
      "source": [
        "# Creating an infusion matrix\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "dataset[\"correct_match\"] = None\n",
        "for index, row in dataset.iterrows():\n",
        "    if row[\"Cluster\"] == 0 and row[\"Is_Bycatch\"] == 1:     # TT\n",
        "        dataset.at[index, \"correct_match\"] = 1\n",
        "    elif row[\"Cluster\"] != 0 and row[\"Is_Bycatch\"] == 0:   # FF\n",
        "        dataset.at[index, \"correct_match\"] = 2\n",
        "    elif  row[\"Cluster\"] == 0 and row[\"Is_Bycatch\"] == 0:  # TF\n",
        "        dataset.at[index, \"correct_match\"] = 3\n",
        "    elif  row[\"Cluster\"] != 0 and row[\"Is_Bycatch\"] == 1:  # FT\n",
        "        dataset.at[index, \"correct_match\"] = 4\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVDpMxS51og0",
        "outputId": "f3bc82dd-1b91-4d62-f171-48f6199d3531"
      },
      "outputs": [],
      "source": [
        "TT = np.sum(dataset[\"correct_match\"] == 1)\n",
        "FF = np.sum(dataset[\"correct_match\"] == 2)\n",
        "TF = np.sum(dataset[\"correct_match\"] == 3)\n",
        "FT = np.sum(dataset[\"correct_match\"] == 4)\n",
        "\n",
        "# Confusion matrix\n",
        "print(\"Number of largest cluster and bycatch:\", TT)\n",
        "print(\"Number of largest cluster and non-bycatch:\", TF)\n",
        "print(\"Number of other clusters and bycatch:\", FT)\n",
        "print(\"Number of other clusters and non-bycatch:\", FF)\n",
        "print(\"Accuracy: {}\".format((TT+FF)/(TT+TF+FT+FF)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOcd7Nvby_oZ"
      },
      "source": [
        "The accuracy shows how much the largest cluster and the other clusters including noise match to bycatch and non-bycatch respectively.\n",
        "\n",
        "According to the outcome, it can be said that the largest cluster can be considered as bycatch and the other groups (the other clusters and noise)  can be interpreted as non-bycatch in the accuracy of 83.7%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc1FpAQCeTUY"
      },
      "source": [
        "#### Reflection on Unsupervised Learning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7g4R18vNhab"
      },
      "source": [
        "We have done a clustering on the data using DBSCAN algorithm, which doesn't need the number of clusters.\n",
        "\n",
        "To find suitable parameters of DBSCAN, we have run DBSCAN with different values of parameters, eps and min_example. In this step, we found that\n",
        "\n",
        "\n",
        "- there is a large number of noise.\n",
        "- increasing eps parameter reduces the size of noise, but the computer RAM is run out of it if we run, so we could not reduce it dramatically; still we have figured out it is getting decreasing.\n",
        "- increasing min_samples parameter reduces the number of clusters. We have got 2 clusters when the min_samples is 200 with the default eps (0.5).\n",
        "- when eps is large, increasing min_samples parameter increases the number of noise.\n",
        "\n",
        "Additionally, we have tried to find a relationship between the result of clustering and those of supervised learning, where we have discovered whether each data point is bycatch or not. We found it intriguing to observe that with numerous clusters, evaluating the algorithm became challenging. This was partly because we discovered similarities in the size of the largest cluster and the group of bycatch. Moreover, the features we employed for clustering were identical to those used to identify bycatch.\n",
        "\n",
        "Through this process, we have concluded that the largest cluster represents bycatch, while the remaining clusters, including the noise, do not, with an accuracy of 83.7%."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ScAwo4_ko4Ow"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "47qY17GaQLxD",
        "BasTf5cEQLxF"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
