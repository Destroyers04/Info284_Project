{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy from main_draft-1sted | intended to copy all maniplution of dataset and \n",
    "# relevant vairables in order to reduse clus when implimented in finale draft\n",
    "\n",
    "import warnings # Got an irritating warning\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"float_format\", \"{:f}\".format)\n",
    "\n",
    "dataset = pd.read_csv(r\"C:\\Users\\Sverre\\Documents\\VS Studio. filer\\INFO284\\Obli\\Info284_Project\\Exam Task\\Dataset\\elektronisk-rapportering-ers-2018-fangstmelding-dca-simple.csv\", sep = \";\")\n",
    "\n",
    "# Dataset where the species isn\"t the same as the main-species\n",
    "bycatch = dataset[dataset[\"Art FAO\"] != dataset[\"Hovedart FAO\"]]\n",
    "\n",
    "# Dataset where the species is the same as the main-species\n",
    "main_species = dataset[dataset[\"Art FAO\"] == dataset[\"Hovedart FAO\"]]\n",
    "\n",
    "# Excluding all features that aren't relevant\n",
    "dataset = dataset[[\"Hovedart FAO\", \"Art FAO\", \"Lengdegruppe\", \"Redskap FAO\", \"Rundvekt\", \"Hovedområde start\"]]\n",
    "\n",
    "# Drop all rows where Hovedart FAO and Art FAO is NaN\n",
    "dataset = dataset.dropna(subset=[\"Hovedart FAO\", \"Art FAO\"])\n",
    "\n",
    "# Make new category in lengdegruppe\n",
    "dataset[\"Lengdegruppe\"] = dataset[\"Lengdegruppe\"].fillna(\"Stortare båter\")\n",
    "\n",
    "# Drop Na values\n",
    "dataset = dataset.dropna(subset=[\"Redskap FAO\"])\n",
    "\n",
    "# Dropping Na values\n",
    "dataset = dataset.dropna(subset=[\"Hovedområde start\"])\n",
    "\n",
    "# Create a new binary feature to use as target variable. \n",
    "dataset[\"Is_Bycatch\"] = (dataset[\"Hovedart FAO\"] != dataset[\"Art FAO\"])\n",
    "\n",
    "# ==========================================================================|\n",
    "\n",
    "# Changing categorical data into numeric data using \"cat.codes\"\n",
    "dataset[\"Art FAO\"] = dataset[\"Art FAO\"].astype(\"category\")\n",
    "dataset[\"Art FAO Codes\"] = dataset[\"Art FAO\"].cat.codes\n",
    "\n",
    "dataset[\"Lengdegruppe\"] = dataset[\"Lengdegruppe\"].astype(\"category\")\n",
    "dataset[\"Lengdegruppe Codes\"] = dataset[\"Lengdegruppe\"].cat.codes\n",
    "\n",
    "dataset[\"Hovedområde start\"] = dataset[\"Hovedområde start\"].astype(\"category\")\n",
    "dataset[\"Hovedområde start Codes\"] = dataset[\"Hovedområde start\"].cat.codes \n",
    "\n",
    "dataset[\"Redskap FAO\"] = dataset[\"Redskap FAO\"].astype(\"category\")\n",
    "dataset[\"Redskap FAO Codes\"] = dataset[\"Redskap FAO\"].cat.codes \n",
    "\n",
    "# Excluding the old features\n",
    "dataset = dataset[[\"Art FAO Codes\", \"Lengdegruppe Codes\", \"Redskap FAO Codes\", \n",
    "                   \"Rundvekt\", \"Hovedområde start Codes\", \"Is_Bycatch\"]]\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting dataset \n",
    "X = dataset[[\"Art FAO Codes\", \"Lengdegruppe Codes\", \"Redskap FAO Codes\", \"Rundvekt\", \"Hovedområde start Codes\"]]\n",
    "y = dataset[\"Is_Bycatch\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=50)\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Scaling only \"Rundvekt\" and not the others\n",
    "ct = ColumnTransformer(\n",
    "    [(\"scale\", RobustScaler(), [\"Rundvekt\"])],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "# Fitting the scaler to the training set and not the test set to prevent data leakage\n",
    "X_train_scaled = ct.fit_transform(X_train)\n",
    "X_test_scaled = ct.transform(X_test)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our choices\n",
    "- Neural network\n",
    "\n",
    "\"Neural networks have a considerable succsess in low-level reasoning for with there is aboundant training data ... one reason is that they are very flexible and can invent features ... as far as learning is conserned, neural network provide a different measure of simplicity as a learning bias then, for eksample, decision trees. Multilayer neural networks, like decision tree, can represent any function of a set of discreat features.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.8829404965888189\n",
      "Accuracy: 0.8815974096060443\n",
      "Confusion Matrix: [[17925  5362]\n",
      " [ 3414 47419]]\n",
      "0.8829404965888189 0.8815974096060443\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mlp = MLPClassifier(verbose=False, hidden_layer_sizes=(10, 5), learning_rate_init=0.001, random_state=50)\n",
    "\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_test_pred = mlp.predict(X_train_scaled)\n",
    "train_accuracy = accuracy_score(y_test_pred, y_train)\n",
    "print(\"Training accuracy: \", train_accuracy)\n",
    "\n",
    "\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print(f\"Confusion Matrix: {confusion_matrix(y_test, y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The low diffeence between the accurasy and training accuracy indicateds that the model is genralsierign well to unseen data and shows little to no signs of overfitting. The low number of false posetiv and false negative indicates that the model is able to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8829404965888189, 0.8829404965888189, 0.8829404965888189, 0.8829404965888189, 0.8829404965888189, 0.8829404965888189, 0.8829404965888189, 0.8829404965888189, 0.8829404965888189, 0.8829404965888189] \n",
      " [0.8815974096060443, 0.8815974096060443, 0.8815974096060443, 0.8815974096060443, 0.8815974096060443, 0.8815974096060443, 0.8815974096060443, 0.8815974096060443, 0.8815974096060443, 0.8815974096060443] \n",
      " [array([[17925,  5362],\n",
      "       [ 3414, 47419]], dtype=int64), array([[17925,  5362],\n",
      "       [ 3414, 47419]], dtype=int64), array([[17925,  5362],\n",
      "       [ 3414, 47419]], dtype=int64), array([[17925,  5362],\n",
      "       [ 3414, 47419]], dtype=int64), array([[17925,  5362],\n",
      "       [ 3414, 47419]], dtype=int64), array([[17925,  5362],\n",
      "       [ 3414, 47419]], dtype=int64), array([[17925,  5362],\n",
      "       [ 3414, 47419]], dtype=int64), array([[17925,  5362],\n",
      "       [ 3414, 47419]], dtype=int64), array([[17925,  5362],\n",
      "       [ 3414, 47419]], dtype=int64), array([[17925,  5362],\n",
      "       [ 3414, 47419]], dtype=int64)] \n",
      " [-0.0013430869827746328, -0.0013430869827746328, -0.0013430869827746328, -0.0013430869827746328, -0.0013430869827746328, -0.0013430869827746328, -0.0013430869827746328, -0.0013430869827746328, -0.0013430869827746328, -0.0013430869827746328]\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "training_acc_list = []\n",
    "testing_acc_list =[]\n",
    "con_x_list = []\n",
    "o_u_fitting_list= []\n",
    "\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    ct = ColumnTransformer(\n",
    "        [(\"scale\", RobustScaler(), [\"Rundvekt\"])],\n",
    "        remainder=\"passthrough\")\n",
    "    X_train_scaled = ct.fit_transform(X_train)\n",
    "    X_test_scaled = ct.transform(X_test)\n",
    "\n",
    "    # aplying split to algo\n",
    "    mlp = MLPClassifier(verbose=False, hidden_layer_sizes=(10, 5), learning_rate_init=0.001, random_state=50)\n",
    "\n",
    "    mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "    y_test_pred = mlp.predict(X_train_scaled)\n",
    "    train_accuracy = accuracy_score(y_test_pred, y_train)\n",
    "    training_acc_list.append(train_accuracy)\n",
    "\n",
    "    y_pred = mlp.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    testing_acc_list.append(accuracy)\n",
    "\n",
    "    o_u_fitting_list.append(accuracy - train_accuracy)\n",
    "    con_x_list.append(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(training_acc_list, \"\\n\", \n",
    "      testing_acc_list, \"\\n\", \n",
    "      con_x_list, \"\\n\", \n",
    "      o_u_fitting_list)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seems that there is little to no sign of biases in the dataset proven by the fackt that the training and testing accursy is indentical across all folds. It is also indendical to the basic implimentation and we then know that the shuffel of the train/test split is sussisent to ensure that there is no large ddifferenses bwtween the training and testing setts, therefor cross validation is not needed in the following grid seartch. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\"learning_rate_init\":[0.01, 0.001, 0.005],\n",
    "              \"activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
    "              \"learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"],\n",
    "              \"hidden_layer_sizes\": [(10, 5), (4, 2), (4, 4), (2, 2)]            \n",
    "} \n",
    "\n",
    "mlp = MLPClassifier(verbose=True, \n",
    "                    hidden_layer_sizes=(10, 5), \n",
    "                    random_state=50)\n",
    "\n",
    "gs = GridSearchCV(estimator=mlp,\n",
    "                  param_grid=parameters,\n",
    "                  n_jobs=-1,\n",
    "                  cv=2,\n",
    "                  verbose=2)\n",
    "\n",
    "gs.fit(X_train_scaled, y_train)\n",
    "best_model = gs.best_estimator_\n",
    "accuracy_best_model = best_model.score(X_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3371424716675661\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam: - stoc grad\n",
    "batch_size\n",
    "learning_rate_init\n",
    "#max_iter\n",
    "#shuffle\n",
    "beta_1\n",
    "beta_2\n",
    "epsilon\n",
    "n_iter_no_change\n",
    "\n",
    "\n",
    "sgd: - stoc grad\n",
    "batch_size\n",
    "learning_rate\n",
    "learning_rate_init\n",
    "power_t\n",
    "#max_iter\n",
    "#shuffle\n",
    "momentum\n",
    "n_iter_no_change\n",
    "\n",
    "\n",
    "lbfgs:\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
