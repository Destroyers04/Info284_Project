{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy from main_draft-1sted | intended to copy all maniplution of dataset and \n",
    "# relevant vairables in order to reduse clus when implimented in finale draft\n",
    "\n",
    "import warnings # Got an irritating warning\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_columns\",None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"float_format\", \"{:f}\".format)\n",
    "\n",
    "aron_path = \"H:\\Informasjonsvitenskap\\Programming\\Python\\Info-284\\Info284_Project\\Exam Task\\Dataset\\elektronisk-rapportering-ers-2018-fangstmelding-dca-simple.csv\"\n",
    "sverre_path = r\"C:\\Users\\Sverre\\Documents\\VS Studio. filer\\INFO284\\Obli\\Info284_Project\\Exam Task\\Dataset\\elektronisk-rapportering-ers-2018-fangstmelding-dca-simple.csv\"\n",
    "\n",
    "dataset = pd.read_csv(sverre_path, sep = \";\")\n",
    "\n",
    "# Dataset where the species isn\"t the same as the main-species\n",
    "bycatch = dataset[dataset[\"Art FAO\"] != dataset[\"Hovedart FAO\"]]\n",
    "\n",
    "# Dataset where the species is the same as the main-species\n",
    "main_species = dataset[dataset[\"Art FAO\"] == dataset[\"Hovedart FAO\"]]\n",
    "\n",
    "# Excluding all features that aren't relevant\n",
    "dataset = dataset[[\"Hovedart FAO\", \"Art FAO\", \"Lengdegruppe\", \"Redskap FAO\", \"Rundvekt\", \"Hovedområde start\"]]\n",
    "\n",
    "# Drop all rows where Hovedart FAO and Art FAO is NaN\n",
    "dataset = dataset.dropna(subset=[\"Hovedart FAO\", \"Art FAO\"])\n",
    "\n",
    "# Make new category in lengdegruppe\n",
    "dataset[\"Lengdegruppe\"] = dataset[\"Lengdegruppe\"].fillna(\"Stortare båter\")\n",
    "\n",
    "# Drop Na values\n",
    "dataset = dataset.dropna(subset=[\"Redskap FAO\"])\n",
    "\n",
    "# Dropping Na values\n",
    "dataset = dataset.dropna(subset=[\"Hovedområde start\"])\n",
    "\n",
    "# Create a new binary feature to use as target variable. \n",
    "dataset[\"Is_Bycatch\"] = (dataset[\"Hovedart FAO\"] != dataset[\"Art FAO\"])\n",
    "\n",
    "# ==========================================================================|\n",
    "\n",
    "# Changing categorical data into numeric data using \"cat.codes\"\n",
    "dataset[\"Art FAO\"] = dataset[\"Art FAO\"].astype(\"category\")\n",
    "dataset[\"Art FAO Codes\"] = dataset[\"Art FAO\"].cat.codes\n",
    "\n",
    "dataset[\"Lengdegruppe\"] = dataset[\"Lengdegruppe\"].astype(\"category\")\n",
    "dataset[\"Lengdegruppe Codes\"] = dataset[\"Lengdegruppe\"].cat.codes\n",
    "\n",
    "dataset[\"Hovedområde start\"] = dataset[\"Hovedområde start\"].astype(\"category\")\n",
    "dataset[\"Hovedområde start Codes\"] = dataset[\"Hovedområde start\"].cat.codes \n",
    "\n",
    "dataset[\"Redskap FAO\"] = dataset[\"Redskap FAO\"].astype(\"category\")\n",
    "dataset[\"Redskap FAO Codes\"] = dataset[\"Redskap FAO\"].cat.codes \n",
    "\n",
    "# Excluding the old features\n",
    "dataset = dataset[[\"Art FAO Codes\", \"Lengdegruppe Codes\", \"Redskap FAO Codes\", \n",
    "                   \"Rundvekt\", \"Hovedområde start Codes\", \"Is_Bycatch\"]]\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Splitting dataset \n",
    "X = dataset[[\"Art FAO Codes\", \"Lengdegruppe Codes\", \"Redskap FAO Codes\", \"Rundvekt\", \"Hovedområde start Codes\"]]\n",
    "y = dataset[\"Is_Bycatch\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=50)\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Scaling only \"Rundvekt\" and not the others\n",
    "ct = ColumnTransformer(\n",
    "    [(\"scale\", RobustScaler(), [\"Rundvekt\"])],\n",
    "    remainder=\"passthrough\"\n",
    ")\n",
    "\n",
    "# Fitting the scaler to the training set and not the test set to prevent data leakage\n",
    "X_train_scaled = ct.fit_transform(X_train)\n",
    "X_test_scaled = ct.transform(X_test)\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our choices\n",
    "- Neural network\n",
    "\"Neural networks have a considerable succsess in low-level reasoning for with there is aboundant training data ... one reason is that they are very flexible and can invent features ... As far as learning is conserned, neural network provide a different measure of simplicity as a learning bias then, for eksample, decision trees. Multilayer neural networks, like decision tree, can represent any function of a set of discreat features.\" [Alan K.Mackworth, David L. Poole (2017). p.309.  \"Artificial intelligence, Foundations of computational agents\". Cambridge, United Kingdom: Cambridge University Press]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic implementation of a neural network and something to compare the testing and fine tuning to come. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.8746430290029097\n",
      "Accuracy: 0.8734754452239611\n",
      "Confusion Matrix: [[17278  6009]\n",
      " [ 3369 47464]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "mlp = MLPClassifier(verbose=False, hidden_layer_sizes=(10, 5), learning_rate_init=0.001, random_state=50)\n",
    "\n",
    "mlp.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_test_pred = mlp.predict(X_train_scaled)\n",
    "train_accuracy = accuracy_score(y_test_pred, y_train)\n",
    "print(\"Training accuracy: \", train_accuracy)\n",
    "\n",
    "\n",
    "y_pred = mlp.predict(X_test_scaled)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "print(f\"Confusion Matrix: {confusion_matrix(y_test, y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are to check if the neural network model is vulnerable to a potential fluctuation in where the train test split is happening. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8968552893426127, 0.8973499881946865, 0.8959221074171099, 0.8935722878697593, 0.9008503573449662, 0.8846789516881598, 0.8973125110089233, 0.8975564966458044, 0.9021361915826557, 0.8933927969118914] \n",
      " [0.8928764166216946, 0.9208378305450621, 0.9098421478683216, 0.8818470048569886, 0.9028939557474366, 0.889840798704803, 0.8842754991905019, 0.8568489223192903, 0.8745572907882754, 0.8949640773096772] \n",
      " [array([[ 7595,  1658],\n",
      "       [ 1518, 18877]], dtype=int64), array([[ 8366,   887],\n",
      "       [ 1460, 18935]], dtype=int64), array([[ 8107,  1146],\n",
      "       [ 1527, 18868]], dtype=int64), array([[ 7304,  1949],\n",
      "       [ 1554, 18841]], dtype=int64), array([[ 7399,  1854],\n",
      "       [ 1025, 19370]], dtype=int64), array([[ 7240,  2013],\n",
      "       [ 1253, 19142]], dtype=int64), array([[ 7223,  2030],\n",
      "       [ 1401, 18994]], dtype=int64), array([[ 6201,  3052],\n",
      "       [ 1192, 19202]], dtype=int64), array([[ 6438,  2814],\n",
      "       [  905, 19490]], dtype=int64), array([[ 7392,  1860],\n",
      "       [ 1254, 19141]], dtype=int64)] \n",
      " [0.003978872720918125, -0.0234878423503756, -0.013920040451211757, 0.011725283012770649, -0.00204359840247037, -0.0051618470166432395, 0.013037011818421429, 0.04070757432651406, 0.0275789007943803, -0.001571280397785757]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nfor i, (train_index, test_index) in enumerate(skf.split(X, y)):\\n    ct = ColumnTransformer(\\n        [(\"scale\", RobustScaler(), [\"Rundvekt\"])],\\n        remainder=\"passthrough\")\\n    X_train_scaled = ct.fit_transform(X_train)\\n    X_test_scaled = ct.transform(X_test)\\n\\n    # aplying split to algo\\n    mlp = MLPClassifier(verbose=False, hidden_layer_sizes=(10, 5), learning_rate_init=0.001, random_state=50)\\n\\n    mlp.fit(X_train_scaled, y_train)\\n\\n    y_test_pred = mlp.predict(X_train_scaled)\\n    train_accuracy = accuracy_score(y_test_pred, y_train)\\n    training_acc_list.append(train_accuracy)\\n\\n    y_pred = mlp.predict(X_test_scaled)\\n    accuracy = accuracy_score(y_test, y_pred)\\n    testing_acc_list.append(accuracy)\\n\\n    o_u_fitting_list.append(accuracy - train_accuracy)\\n    con_x_list.append(confusion_matrix(y_test, y_pred))\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=10)\n",
    "\n",
    "training_acc_list = []\n",
    "testing_acc_list =[]\n",
    "con_x_list = []\n",
    "o_u_fitting_list= []\n",
    "\n",
    "\n",
    "for train_index, test_index in skf.split(X, y):\n",
    "\n",
    "    X_train_fold = X.iloc[train_index]\n",
    "    X_test_fold = X.iloc[test_index]\n",
    "    y_train_fold = y.iloc[train_index]\n",
    "    y_test_fold = y.iloc[test_index]\n",
    "\n",
    "    X_train_fold_scaled = ct.fit_transform(X_train_fold)\n",
    "    X_test_fold_scaled = ct.transform(X_test_fold)\n",
    "\n",
    "    mlp.fit(X_train_fold_scaled, y_train_fold)\n",
    "\n",
    "    training_acc_list.append(mlp.score(X_train_fold_scaled, y_train_fold))\n",
    "    testing_acc_list.append(mlp.score(X_test_fold_scaled, y_test_fold))\n",
    "    con_x_list.append(confusion_matrix(y_test_fold, mlp.predict(X_test_fold_scaled)))\n",
    "    train_test_diff = (mlp.score(X_train_fold_scaled, y_train_fold)) - (mlp.score(X_test_fold_scaled, y_test_fold))\n",
    "    o_u_fitting_list.append(train_test_diff)\n",
    "\n",
    "\n",
    "print(training_acc_list, \"\\n\", \n",
    "      testing_acc_list, \"\\n\", \n",
    "      con_x_list, \"\\n\", \n",
    "      o_u_fitting_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output's second line highlights a significant gap in prediction performance, with nearly a 7% point difference between the lowest (~85.6) and highest (~92.0) correct prediction rates. This suggests that the predictor is sensitivity to how the data splitting occurs. Without shuffling, it may indicate a concentration of certain data aspects, while with sufficient shuffling, it may reveal more entries with a specific label.\n",
    "\n",
    "In 8/10 interactions, the confusion matrix shows a higher number of false positives than false negatives. But the values of the confusion matrix appear to be somewhat volatile and fluctuate within a range of a few hundred, so this is something to keep in mind for the future. \n",
    "\n",
    "\n",
    "In the last line of the output, you can see the difference between prediction accuracy on the training and test data. Half of the numbers are negative, indicating that in this cross-validation iteration, the model performs worse on the training data than on the test data. Possible reasons for this could include data leakage, significant differences in the data splitting, or a model structure that isn't complex enough to fit the training data effectively.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the best parameters for the neural network, we conducted a grid search. Early stopping was enabled to use a portion of the training dataset as a validation set, stopping training if no improvement is observed. Cross-validation with F1 scoring and pipeline were employed to address potential issues mentioned earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "parameters = {\"neural_net__activation\": [\"identity\", \"logistic\", \"tanh\", \"relu\"],\n",
    "              \"neural_net__solver\": [\"lbfgs\", \"sgd\", \"adam\"],\n",
    "              \"neural_net__learning_rate\": [\"constant\", \"invscaling\", \"adaptive\"],\n",
    "              \"neural_net__learning_rate_init\":[0.01, 0.001, 0.0001],\n",
    "              \"neural_net__hidden_layer_sizes\": [(100,), (50,), (10,), (5,), (1,),  \n",
    "                                                 (100, 50), (50, 50), (10, 5), (4, 2), (2, 2), ]      \n",
    "} \n",
    "\n",
    "mlp = MLPClassifier(early_stopping=True, random_state=50)\n",
    "\n",
    "pipe = Pipeline([(\"scaler\", ct), (\"neural_net\", mlp)])\n",
    "\n",
    "grid = GridSearchCV(pipe, \n",
    "                    param_grid=parameters, \n",
    "                    cv=10, \n",
    "                    verbose=1, \n",
    "                    n_jobs=-1,\n",
    "                    scoring=make_scorer(f1_score))\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Test set score: {:.2f}\".format(grid.score(X_test, y_test)))\n",
    "print(\"Best parameters: {}\".format(grid.best_params_))\n",
    "best_model = grid.best_estimator_\n",
    "\n",
    "#[output] >>>\n",
    "\n",
    "\"\"\"\n",
    "Fitting 10 folds for each of 1512 candidates, totalling 15120 fits\n",
    "\n",
    "Best cross-validation accuracy: 0.94\n",
    "Test set score: 0.94\n",
    "Best parameters: {'neural_net__activation': 'tanh', \n",
    "'neural_net__hidden_layer_sizes': (100, 50), \n",
    "'neural_net__learning_rate': 'constant', \n",
    "'neural_net__learning_rate_init': 0.001, \n",
    "'neural_net__solver': 'adam'}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After runing the python cell above we got the following output:\n",
    "\n",
    "[output] >>>\n",
    "\n",
    "Fitting 10 folds for each of 1512 candidates, totalling 15120 fits\n",
    "\n",
    "Best cross-validation accuracy: 0.94\n",
    "Test set score: 0.94\n",
    "Best parameters: {'neural_net__activation': 'tanh', \n",
    "'neural_net__hidden_layer_sizes': (100, 50), \n",
    "'neural_net__learning_rate': 'constant', \n",
    "'neural_net__learning_rate_init': 0.001, \n",
    "'neural_net__solver': 'adam'}\n",
    "___________________________________________________\n",
    "\n",
    "After grid searching some parameters, we found that the above parameters yielded the best model performance. However, fine-tuning only resulted in a 2% point improvement compared to the best cross-validation integration for the basic implementation. This change is relatively small, especially considering the larger impact the data splitting had on performance. It's possible that this minor improvement is due to the high accuracy of the model, and the remaining mispredictions are deviating slightly from the overall pattern.\n",
    "\n",
    "To gain the final percentage points, further fine-tuning of the hidden layer is likely necessary. The output indicates that the most complex model performed the best, suggesting that the current model might be too simplistic to capture the underlying pattern effectively. Further testing is needed to confirm this hypothesis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same parameters as with the best performing model from the gridsearch, we made some changes to how the hidden layers were defined. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n",
      "Best cross-validation accuracy: 0.95\n",
      "Test set score: 0.95\n",
      "Best parameters: {'neural_net__activation': 'tanh', 'neural_net__hidden_layer_sizes': (100, 100, 100), 'neural_net__learning_rate': 'constant', 'neural_net__learning_rate_init': 0.001, 'neural_net__solver': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "arameters = {\"neural_net__activation\": [\"tanh\"],\n",
    "              \"neural_net__solver\": [\"adam\"],\n",
    "              \"neural_net__learning_rate\": [\"constant\"],\n",
    "              \"neural_net__learning_rate_init\":[0.001],\n",
    "              \"neural_net__hidden_layer_sizes\": [(100, 50), (100, 100),\n",
    "                                                 (10, 10, 10), (50, 50, 50), (100, 100, 100),\n",
    "                                                 (50, 50, 50, 50)]      \n",
    "} \n",
    "\n",
    "mlp = MLPClassifier(early_stopping=True, random_state=50)\n",
    "\n",
    "pipe = Pipeline([(\"scaler\", ct), (\"neural_net\", mlp)])\n",
    "\n",
    "grid = GridSearchCV(pipe, \n",
    "                    param_grid=arameters, \n",
    "                    cv=10, \n",
    "                    verbose=1, \n",
    "                    n_jobs=-1,\n",
    "                    scoring=make_scorer(f1_score))\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Test set score: {:.2f}\".format(grid.score(X_test, y_test)))\n",
    "print(\"Best parameters: {}\".format(grid.best_params_))\n",
    "best_model = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[output] >>> \n",
    "\n",
    "Fitting 10 folds for each of 6 candidates, totalling 60 fits\n",
    "\n",
    "Best cross-validation accuracy: 0.95\n",
    "\n",
    "Test set score: 0.95\n",
    "\n",
    "Best parameters: {'neural_net__activation': 'tanh', 'neural_net__hidden_layer_sizes': (100, 100, 100), 'neural_net__learning_rate': 'constant', 'neural_net__learning_rate_init': 0.001, 'neural_net__solver': 'adam'}\n",
    "\n",
    "_______________________________________________________________\n",
    "\n",
    "The model's 1% point improvement confirms our theory. Despite an alternative parameter with more hidden layers but fewer neurons, it may not have been a more complex or suitable model. If we were to wish to improve the model any more in the future, a lot more exploring and fine tuning on this aspect of the model would be needed. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
